---
title: "Dada2 Workflow"
author: "Eric Malekos"
date: "April 23, 2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

#### Information Packed Tutorials:
https://benjjneb.github.io/dada2/tutorial.html - The official Dada2 Tutorial  
https://astrobiomike.github.io/amplicon/dada2_workflow_ex - A second helpful walkthrough  
**Most of the code that follows is a reproduction of code that was originally written up in these two sources**


Note, some of the methods take a while and you may not want to run the whole script at once, or you may want to restart from a certain point in the pipeline. To accomplish this you can use the the R methods saveRDS and readRDS. This allows you to save any environmental variable and reload it in the future without worrying about formats.  
**Format:**  
```{r, eval=F, echo=T} 
saveRDS([variableToSave], [path/filename])  

[variableName] = readRDS([path/filename])  
```
**Example:**  
```{r, eval=F, echo=T} 
errF <- learnErrors(R1_filtFs, multithread=TRUE)  
saveRDS(errF, "/intermediateData/errF.rds")  

reloaded_errF <- readRDS("/intermediateData/errF.rds")  
```

## Setting Up Environment
Install the dada2 package:
```{r installation, eval=FALSE, echo =T}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("dada2", version = "3.10")
```

```{r setPaths, echo=T}
library(dada2)

path = "/home/eric/projects/Microbiome"
dataFolder = "/Project_JA14487"

if(getwd() != path)
  setwd(path)

dataPath = paste0(path,dataFolder)
head(list.files(dataPath))

```

### The Data
Below are the first two sequence entries from one of the fastq files. Each entry is composed of four lines:  
1. Sequence metadata  
2. The sequence  
3. "+"" sign  
4. The Quality scores  

```{r showData, warning=FALSE}
con <- file(paste0(dataPath,"/",list.files(dataPath)[3]),"r")
print(readLines(con,n=8))
close(con)
```
### Quality Scores  

Each nucleotide in a sequence has a corresponding Quality score (AKA Phred Score) according to the equation below  

**P** := the probability of an erroneously placed base  
**Q** := the Quality/Phred score  

$$P = 10^{\frac{-Q}{10}}$$
$$Q = -10 log_{10}(P)$$

#### Examples:

```{r phredScores}

P = c(0.01, 0.001, 0.0001)
for (p in P){print(paste0("For P = ", p, " , Q = ", -10*log10(p)))}
```

### Quality Profiles

Here we can assess the quality of our sequences visually. Plotted are two of the forward reads and two of the reverse reads. Reverse reads are generally of lower quality.
```{r qualityProfiles, cache=TRUE}
R1_fnFs <- sort(list.files(dataPath, pattern="_R1_001.fastq", full.names = TRUE))
R2_fnRs <- sort(list.files(dataPath, pattern="_R2_001.fastq", full.names = TRUE))

sample.names <- sapply(strsplit(basename(R1_fnFs), "_"), `[`, 1)

plotQualityProfile(R1_fnFs[1:2])
plotQualityProfile(R2_fnRs[1:2])
```

### Quality Filtering

Here we filter the data to prepare it for the merge step.

```{r filter, echo=FALSE, cache=TRUE}
R1_filtFs <- file.path(dataPath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
R2_filtRs <- file.path(dataPath, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(R1_filtFs) <- sample.names
names(R2_filtRs) <- sample.names

```

```{r trim, cache=TRUE}
out <- filterAndTrim(R1_fnFs, R1_filtFs, R2_fnRs, R2_filtRs, truncLen=c(240,200),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

```

* **truncLen** := the truncation length of the reads. The first number is the portion of the forward
                  read we want to keep - in this case we are keeping 240/250 nucleotides from the forward read.
                  Likewise we're keeping 200/250 from the reverse reads. (more below)

* **maxN** := Ns are from illumina sequencing, dada2 doesn't work with them so set this to 0
* **maxEE** := "expected errors" reads with more expected errors are discarded (evaluated after truncating)
* **truncQ** := truncate reads at the first quality score of 
* **rm.phix** := discard reads that match against the phiX genome
* **compress** := compress the trimmed reads
* **multithread** := make your computer work harder - doesn't work on Windows

In my very limited experience truncLen has been the most infuential parameter, some trimming is important to remove messy sequences but too much trimming can lead 


### Merging
 
```{r out, results=FALSE, cache=TRUE}
errF <- learnErrors(R1_filtFs, multithread=TRUE)
errR <- learnErrors(R2_filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)

```

```{r merge, eval=TRUE, echo=FALSE, cache=TRUE}
#Sample Inference
dadaFs <- dada(R1_filtFs, err=errF, multithread=TRUE, verbose = FALSE)
dadaRs <- dada(R2_filtRs, err=errR, multithread=TRUE, verbose = FALSE)

mergers <- mergePairs(dadaFs, R1_filtFs, dadaRs, R2_filtRs, verbose=FALSE)
```

### Merge Results
The columns of the following table are the lengths, in nucleotides, of the merged sequences and the entries are the number of seqeunces of that length.  
**Sanity Check** - Compare the distribution of merge lengths to what you would expect based on the length of the sequence you amplified. Are they as expected? If not you may need to return to the filterAndTrim step and adjust the parameters.
```{r table, echo=FALSE, cache=TRUE}
seqtab <- makeSequenceTable(mergers)
table(nchar(getSequences(seqtab)))
```


### Removing Chimeras

We remove Chimeras then check the ratio of non-chimera sequences to total merged sequences. The closer this ratio is to 1.0 the better.
```{r chimeras, cache=TRUE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim)/sum(seqtab)
```
### Check for loss: 
Here we can get an idea of how many sequences we lost in each of the above steps.
```{r lost, cache=TRUE}
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=sample.names, dada2_input=out[,1],
                          filtered=out[,2], dada_f=sapply(dadaFs, getN),
                          dada_r=sapply(dadaRs, getN), merged=sapply(mergers, getN),
                          nonchim=rowSums(seqtab.nochim),
                          final_percent=round(rowSums(seqtab.nochim)/out[,1]*100, 1))

summary_tab
```

### Assigning Taxa
We assign taxonomic labels to our data by cross referencing it with labelled data. You can find multiple datasets to reference against here:https://benjjneb.github.io/dada2/training.html

Here we are going to use the Silva version 138 set. To go there directly: https://zenodo.org/record/3731176.

From here we need to download into our working directory:
* silva_nr_v138_train_set.fa.gz
* silva_species_assignment_v138.fa.gz

```{r assign, cache=TRUE}
taxa <- assignTaxonomy(seqtab.nochim, paste0(dataPath,"/IDTaxa/silva_nr_v132_train_set.fa.gz"), multithread=TRUE)
summary(taxa)
```


From the summary output we can see that some of the sequences are not ones we're interested in: Eukaryotes, Mitochondria and Chloroplasts. We'll remove them **We need to remove from the Taxa Table and the Sequence Tabel**.


```{r remove,cache=TRUE}


is.mito <- taxa[,"Family"] %in% "Mitochondria"
is.chloro <- taxa[,"Order"] %in% "Chloroplast"
is.euk <- taxa[,"Kingdom"] %in% "Eukaryota"

to.remove <- Reduce("|", list(is.mito,is.chloro, is.euk))
taxa.trimmed <- taxa[!to.remove,]
seqtab.trimmed <- seqtab.nochim[,!to.remove]
summary(taxa.trimmed)
```

### Assign Species

Next we expand classification to the species level. We only match 17 species. Oh well.
```{r species, cache=TRUE}
taxa.final <- addSpecies(taxa.trimmed, paste0(dataPath,"/IDTaxa/silva_species_assignment_v132.fa.gz"))
taxa.print <- taxa.final # Removing sequence rownames for display only
temp <- taxa.final 
rownames(temp)  <- NULL
species<- na.omit(temp[,"Species"])
head(species, length(species))
```

### Exporting the Data

This is a good point to save our results before getting into data analysis. Comma Seperated Values (csv) or Tab Seperated Values are a portable format for large tables. I prefer TSVs.
```{r export, eval=FALSE}
write.table(taxa.final, "Silva_Assigned.tsv", sep = "\t", quote=FALSE, col.names = NA)

asv_seqs <- colnames(seqtab.trimmed)
asv_headers <- vector(dim(seqtab.trimmed)[2], mode="character")

for (i in 1:dim(seqtab.trimmed)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

# count table:
asv_tab <- t(seqtab.trimmed)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

asv_tax <- taxa.final
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)

```

### Epilogue: Normalization Across Samples
(You may prefer to include this in a separate data analysis script)

For normalization we're going to use the DESeq2 package
```{r deseq, eval=FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")
```
```{r deseq2, echo=FALSE}
library("DESeq2")
```

We're going to re-import our data and bring in a new file - "sample_treatment.tsv". This file is a metadate file with samples and their experimental treatment. Below we show a few random entries from this file.

```{r sample}
count_tab <- read.table("ASVs_counts.tsv", header=T, row.names=1,
                        check.names=F, sep="\t")

tax_tab <- as.matrix(read.table("ASVs_taxonomy.tsv", header=T,
                                row.names=1, check.names=F, sep="\t"))

sample_info_tab <- read.table("sample_treatment.tsv", header=T, row.names=1,
                              check.names=F, sep="\t")
sample_info_tab

```

## Important Data Preparation Checks


### First Important Check
We may want to remove ASVs which  show up in insubstantial quanttiy. For example if there are only one or two instances of an ASV across all samples, we might toss it out. Below we remove all ASVs that only show up once.
```{r second}
minimum_threshold = 1
count_tab = count_tab[rowSums(count_tab) > minimum_threshold, ]
```

### Second Important Check
We need to make sure the column names from "count_tab" are matched to row_names from "sample_treatment.tsv". That means the sample names must be **exactly** the same (spelling, capitalization, order). Here we make sure the tables are sorted in the same order and check that the names match. 
```{r first}
sample_info_tab = sample_info_tab[order(row.names(sample_info_tab)), ]
count_tab = count_tab[ , order(names(count_tab))]

print(colnames(count_tab) == rownames(sample_info_tab)) # This has to return TRUE for all entries, if it doesnt make sure sample names are the same
```

### Third Imporant Check
For some statistical analyses (e.g. Bray-Curtis) we cannot have sample columns that consist entirely of zeroes - i.e. experimental samples which contain no merged sequences. Let's remove those from count_tab and the corresponding entries from sample_info_tab
```{r third}
nonZeroes = count_tab[, colSums(count_tab) != 0]
count_tab = count_tab[, colnames(nonZeroes)]
sample_info_tab = sample_info_tab[colnames(nonZeroes),  ]
```

### Normalize
Now we're ready to normalize
In the DESeqDataSetFromMatrix, make sure that the "design" parameter is set against the variable of interest from the sample metadata file. In this project that information is stored in the "Treatment" variable. If there are multiple variables of interest you can use: design ~ var1 + var2 + var3 + ...
```{r runit, cache=TRUE}
deseq_counts <- DESeqDataSetFromMatrix(countData = count_tab, colData = sample_info_tab, design = ~ Treatment) 

deseq_counts <- estimateSizeFactors(deseq_counts, type = "poscount") #do this step when there are lots of "0" entries
deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)

# and here is pulling out our transformed table
vst_trans_count_tab <- assay(deseq_counts_vst)
```


### Negative Values
The normalization steps above will produce both positive and negative values. Some downstream analysis methods require positive values. We can shift our values so that they are all positive, while maintaining the distribution, by adding the absolute value of the most negative number:
```{r neg}
vst_trans_count_tab.2 <- ceiling(abs(min(vst_trans_count_tab))) + vst_trans_count_tab
```

